{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f3d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a33b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import numpy\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2781a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    with open(sys.argv[1], 'r', encoding='utf-8') as train:\n",
    "        trainData = train.readlines(\"train.txt\") # copy the content of the file in a list\n",
    "\n",
    "    with open(sys.argv[2], 'r', encoding='utf-8') as test:\n",
    "        testData = test.readlines(\"test.txt\")\n",
    "\n",
    "    return trainData, testData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8c7b0b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-3179bc21cada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'trainData' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fbdbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemmer(doc):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    roots = [stemmer.stem(plural) for plural in doc]\n",
    "\n",
    "    return roots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c52317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_corpus(data, use_sentiment):\n",
    "\n",
    "    documents = []\n",
    "    labels = []\n",
    "\n",
    "    for line in data:\n",
    "        tokens = line.strip().split()  # tokenize the lines\n",
    "\n",
    "        documents.append(tokens[3:])  # append the text - starts from 4th tokens\n",
    "\n",
    "        if use_sentiment:\n",
    "            # 2-class problem: positive vs negative\n",
    "            labels.append(tokens[1])  # tokens[1] is sentiment type (either pos/neg)\n",
    "        else:\n",
    "            # 6-class problem: books, camera, dvd, health, music, software\n",
    "            labels.append(tokens[0])  # tokens[0] is one of 6 topic types\n",
    "\n",
    "    stemmed_documents = []\n",
    "    for doc in documents:\n",
    "        stemmed_documents.append(apply_stemmer(doc))\n",
    "\n",
    "    return stemmed_documents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "148e370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Distribution of Data\n",
    "def distribution(trainClass, testClass):\n",
    "\n",
    "    labels = [\"books\", \"camera\", \"dvd\", \"health\", \"music\", \"software\"]\n",
    "    count_training = [0, 0, 0, 0, 0, 0]\n",
    "    count_testing = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    i = 0\n",
    "    for label in labels:\n",
    "        for cls in trainClass:\n",
    "            if cls == label:\n",
    "                count_training[i] += 1\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    for label in labels:\n",
    "        for cls in testClass:\n",
    "            if cls == label:\n",
    "                count_testing[i] += 1\n",
    "        i += 1\n",
    "\n",
    "    print(\"Distribution of classes in Training Set:\")\n",
    "    print(labels)\n",
    "    print(count_training)\n",
    "\n",
    "    print(\"\\nDistribution of classes in Testing Set:\")\n",
    "    print(labels)\n",
    "    print(count_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ce301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dummy function that just returns its input\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "# Using NLTK lemmatizer\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e03b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_func(tfidf, stopwords_bn):\n",
    "    # let's use the\n",
    "\n",
    "    # we use a dummy function as tokenizer and preprocessor,\n",
    "    # since the texts are already preprocessed and tokenized.\n",
    "    # if tfidf:\n",
    "    #     vec = TfidfVectorizer(stop_words=stopwords_bn, preprocessor = identity, tokenizer = identity, ngram_range=(1, 3))\n",
    "    # else:\n",
    "    #     vec = CountVectorizer(stop_words=stopwords_bn, preprocessor = identity, tokenizer = identity, ngram_range=(1, 3))\n",
    "\n",
    "    # using lemmatizer doesn't improve performance\n",
    "    if tfidf:\n",
    "        vec = TfidfVectorizer(preprocessor = identity, tokenizer = identity)\n",
    "    else:\n",
    "        vec = CountVectorizer(preprocessor = identity, tokenizer = identity)\n",
    "\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd472997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_classifier(trainDoc, trainClass, testDoc, testClass, stopwords_bn, tfIdf, use_sentiment):\n",
    "\n",
    "    # decides on TfidfVectorizer(True) or CountVectorizer(False)\n",
    "    vec = tf_idf_func(tfIdf, stopwords_bn)\n",
    "\n",
    "    # combine the vectorizer with a Naive Bayes classifier\n",
    "    classifier = Pipeline( [('vec', vec),\n",
    "                            ('cls', MultinomialNB())] )\n",
    "\n",
    "    t0 = time.time()\n",
    "    # Fit/Train Multinomial Naive Bayes classifier according to trainDoc, trainClass\n",
    "    # Here trainDoc are the documents from training set and trainClass is the class labels for those documents\n",
    "    classifier.fit(trainDoc, trainClass)\n",
    "\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    t1 = time.time()\n",
    "    # Use the classifier to predict the class for all the documents in the test set testDoc\n",
    "    # Save those output class labels in testGuess\n",
    "    testGuess = classifier.predict(testDoc)\n",
    "\n",
    "    test_time = time.time() - t1\n",
    "\n",
    "    # Just to know the output type\n",
    "    classType = \"Topic Class\"\n",
    "    if use_sentiment:\n",
    "        classType = \"Sentiment Class\"\n",
    "\n",
    "    # Just to know which version of Tfidf is being used\n",
    "    tfIDF_type = \"TfidfVectorizer\" if(tfIdf) else \"CountVectorizer\"     # This is ternary conditional operator in python\n",
    "\n",
    "    print(\"\\n########### Naive Bayes Classifier For \", classType, \" (\", tfIDF_type, \") ###########\")\n",
    "\n",
    "    # Call to function(s) to do the jobs ^_^\n",
    "    calculate_measures(classifier, testClass, testGuess)\n",
    "\n",
    "    # Showing 10 fold cross validation score cv = no. of folds\n",
    "    # print(\"Cross Validation:\\n\", cross_val_score(classifier, testDoc, testClass, cv=10))\n",
    "    print()\n",
    "    print(\"Training Time: \", train_time)\n",
    "    print(\"Testing Time: \", test_time)\n",
    "\n",
    "    calculate_probabilities(classifier, testClass, trainClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4f0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4505dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
